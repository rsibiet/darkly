********** Robots.txt exploits **********

URL :
- http://192.168.145.128/robots.txt

Preview :
- Website owners use the /robots.txt file to give instructions about their site to web robots. This is called The Robots Exclusion Protocol. Those files are use by search engines spiders like Google to help referencing the website on the search engine.

Steps :
- Go to url http://192.168.145.128/robots.txt
- The "User-agent": * means this section applies to all robots.
- The "Disallow: /" tells the robot that it should not visit any pages on the site.
- Here we have 2 paths disallowed
- First is, /whatever
- Second is, /.hidden
- Going to index /whatever we can download a file call: htpasswd
It content: root:8621ffdbc5698829397d97767ac13db3
- Convert the string using md5 hash. The result is : dragon
- Go to the admin page: http://192.168.145.128/admin/
- Log with:
	-> Username: root
	-> Password: dragon

Fix :
- Restrict access and protect the paths written in robots.txt
- Make sure those files don't contain any sensible informations

Flag: d19b4823e0d5600ceed56d5e896ef328d7a2b9e7ac7e80f4fcdb9b10bcb3e7ff
